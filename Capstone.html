<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Brett Wiens" />

<meta name="date" content="2018-02-22" />

<title>Capstone</title>

<script src="Capstone_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Capstone_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Capstone_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Capstone_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Capstone_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="Capstone_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="Capstone_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="Capstone_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="Capstone_files/navigation-1.1/tabsets.js"></script>
<link href="Capstone_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Capstone_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Capstone</h1>
<h4 class="author"><em>Brett Wiens</em></h4>
<h4 class="date"><em>February 22, 2018</em></h4>

</div>


<div id="natural-language-analysis-of-a-corpora-of-text" class="section level2">
<h2>Natural Language Analysis of a Corpora of Text</h2>
<p>We have a large amount of text data provided from news, blogs, and twitter. We would like to create a program that will utilize this body (corpus) of data to receive input from a user in the form of a short amount of text and from that predict for them the likely next words. This kind of technology is frequently used on mobile devices and other predictive text. In this markdown file we will explore the data, process and clean it so that we can predict with some confidence what the user would like to see.</p>
</div>
<div id="data-acquisition" class="section level2">
<h2>Data Acquisition</h2>
<p>Assuming that all the source data has already been downloaded, I’m going to start by setting the working directory and pointing to the data. We will then read the data from the text files for analysis.</p>
<pre class="r"><code>## Here are the locations for each of the source datasets, rather than hard-coding... ever.  I&#39;m going to use these to connect whenever I need &#39;em.
dataEnglishBlogs &lt;- &quot;./Data/final/en_US/en_US.blogs.txt&quot;
dataEnglishTwitter &lt;- &quot;./Data/final/en_US/en_US.twitter.txt&quot;
dataEnglishNews &lt;- &quot;./Data/final/en_US/en_US.news.txt&quot;

## Read each file line by line into R
Twitter &lt;- readLines(con &lt;- file(dataEnglishTwitter, &quot;r&quot;), skipNul = TRUE)
close(con)  ## Clean Up After Myself
Blogs &lt;- readLines(con &lt;- file(dataEnglishBlogs, &quot;r&quot;), skipNul = TRUE)
close(con)  ## Clean Up After Myself
News &lt;- readLines(con &lt;- file(dataEnglishNews, &quot;r&quot;), skipNul = TRUE)
close(con)  ## Clean Up After Myself</code></pre>
</div>
<div id="exploring-the-data" class="section level2">
<h2>Exploring the Data</h2>
<p>Lets take a minute to put on our explorer’s hats and learn about the data:</p>
<pre class="r"><code>## Word Counts
twitWord &lt;- sum(stri_count_words(Twitter))
blogWord &lt;- sum(stri_count_words(Blogs))
newsWord &lt;- sum(stri_count_words(News))

## File Size
twitSize &lt;- paste(round(file.info(dataEnglishTwitter)$size / 1024 / 1024,digits = 2), &quot;MB&quot;, sep = &quot; &quot;)
blogSize &lt;- paste(round(file.info(dataEnglishBlogs)$size / 1024 / 1024, digits = 2), &quot;MB&quot;, sep = &quot; &quot;)
newsSize &lt;- paste(round(file.info(dataEnglishNews)$size / 1024 / 1024, digits = 2), &quot;MB&quot;, sep = &quot; &quot;)

## Line Counts
twitLines &lt;- length(Twitter)  ## How many lines in Twitter file (they were read in line by line, so this is pretty easy)
blogLines &lt;- length(Blogs)    ## How many lines in Blogs file (they were read in line by line, so this is pretty easy)
newsLines &lt;- length(News)     ## How many lines in News file (they were read in line by line, so this is pretty easy)

## Maximum words per Line
twitWPL &lt;- max(sapply(gregexpr(&quot;\\W+&quot;, Twitter), length) + 1)   ## \\W+ searches for non-word character \\w+ searches for word character
blogWPL &lt;- max(sapply(gregexpr(&quot;\\W+&quot;, Blogs), length) + 1)     ## \\W+ searches for non-word character \\w+ searches for word character
newsWPL &lt;- max(sapply(gregexpr(&quot;\\W+&quot;, News), length) + 1)      ## \\W+ searches for non-word character \\w+ searches for word character

## Average Words/Line
twitMean &lt;- mean(twitWord)
blogMean &lt;- mean(blogWord)
newsMean &lt;- mean(newsWord)

exploratoryData &lt;- data.frame(c(&quot;Twitter&quot;, &quot;Blogs&quot;, &quot;News&quot;), c(twitWord, blogWord, newsWord), c(twitSize, blogSize, newsSize), 
                             c(twitLines, blogLines, newsLines), c(twitWPL, blogWPL, newsWPL), c(twitMean, blogMean,newsMean ))
names(exploratoryData) &lt;- c(&quot;Source&quot;, &quot;Total Words&quot;, &quot;Size&quot;, &quot;Total&quot;, &quot;Maximum Words &lt;/br&gt; per Line&quot;, &quot;Average Words &lt;/br&gt; per Line&quot;)

exploratoryData[,-1] %&gt;% htmlTable(header = names(exploratoryData[,-1]), 
                              align = &quot;rr|rrr&quot;, 
                              n.cgroup = c(2,3), 
                              cgroup = c(&quot;File&quot;, &quot;Lines&quot;),
                              rnames = exploratoryData$Source,
                              col.columns = c(rep(&quot;#F3E1D8&quot;,1), rep(&quot;#FFFFE0&quot;,1)))</code></pre>
<table class="gmisc_table" style="border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;">
<thead>
<tr>
<th style="border-top: 2px solid grey;">
</th>
<th colspan="2" style="font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
File
</th>
<th style="border-top: 2px solid grey;; border-bottom: hidden;">
 
</th>
<th colspan="3" style="font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Lines
</th>
</tr>
<tr>
<th style="border-bottom: 1px solid grey;">
</th>
<th style="border-bottom: 1px solid grey; text-align: center;">
Total Words
</th>
<th style="border-bottom: 1px solid grey; text-align: center;">
Size
</th>
<th style="border-bottom: 1px solid grey;" colspan="1">
 
</th>
<th style="border-bottom: 1px solid grey; text-align: center;">
Total
</th>
<th style="border-bottom: 1px solid grey; text-align: center;">
Maximum Words </br> per Line
</th>
<th style="border-bottom: 1px solid grey; text-align: center;">
Average Words </br> per Line
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">
Twitter
</td>
<td style="text-align: right; background-color: #f3e1d8;">
30218166
</td>
<td style="border-right: 1px solid black; text-align: right; background-color: #ffffe0;">
159.36 MB
</td>
<td style colspan="1">
 
</td>
<td style="text-align: right; background-color: #f3e1d8;">
2360148
</td>
<td style="text-align: right; background-color: #ffffe0;">
63
</td>
<td style="text-align: right; background-color: #f3e1d8;">
30218166
</td>
</tr>
<tr>
<td style="text-align: left;">
Blogs
</td>
<td style="text-align: right; background-color: #f3e1d8;">
38154238
</td>
<td style="border-right: 1px solid black; text-align: right; background-color: #ffffe0;">
200.42 MB
</td>
<td style colspan="1">
 
</td>
<td style="text-align: right; background-color: #f3e1d8;">
899288
</td>
<td style="text-align: right; background-color: #ffffe0;">
6852
</td>
<td style="text-align: right; background-color: #f3e1d8;">
38154238
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid grey; text-align: left;">
News
</td>
<td style="border-bottom: 2px solid grey; text-align: right; background-color: #f3e1d8;">
2693898
</td>
<td style="border-bottom: 2px solid grey; border-right: 1px solid black; text-align: right; background-color: #ffffe0;">
196.28 MB
</td>
<td style="border-bottom: 2px solid grey;" colspan="1">
 
</td>
<td style="border-bottom: 2px solid grey; text-align: right; background-color: #f3e1d8;">
77259
</td>
<td style="border-bottom: 2px solid grey; text-align: right; background-color: #ffffe0;">
1522
</td>
<td style="border-bottom: 2px solid grey; text-align: right; background-color: #f3e1d8;">
2693898
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>rm(twitWord, blogWord, newsWord, twitSize, blogSize, newsSize, twitLines, blogLines, newsLines, twitWPL, blogWPL, newsWPL, twitMean, blogMean,newsMean, exploratoryData )</code></pre>
</div>
<div id="combine-and-sample-the-data" class="section level2">
<h2>Combine and Sample the Data</h2>
<p>Because we have a lot of data here, I chose to sample the data at a percent desired. A smaller sample will make it faster, but will reduce the total accuracy.</p>
<pre class="r"><code>require(textclean)</code></pre>
<pre><code>## Loading required package: textclean</code></pre>
<pre class="r"><code>## Define the sample amount
SamplePercent = 1
SamplePercent = SamplePercent/100

dfBlogs &lt;- data_frame(source = &quot;Blog&quot;, text = Blogs)
dfNews &lt;- data_frame(source = &quot;News&quot;, text = News)
dfTwitter &lt;- data_frame(source = &quot;Twitter&quot;, text = Twitter)

## Take some samples
SampleBlog &lt;- sample(dfBlogs$text, length(dfBlogs$text) * SamplePercent)
SampleNews &lt;- sample(dfNews$text, length(dfNews$text) * SamplePercent)
SampleTwitter &lt;- sample(dfTwitter$text, length(dfTwitter$text) * SamplePercent)

## Combine the Samples
dfSampleBlog &lt;- data_frame(source = &quot;Blog&quot;, text = SampleBlog)
dfSampleNews &lt;- data_frame(source = &quot;News&quot;, text = SampleNews)
dfSampleTwitter &lt;- data_frame(source = &quot;Twitter&quot;, text = SampleTwitter)

Samples &lt;- rbind(dfSampleBlog, dfSampleNews, dfSampleTwitter)</code></pre>
</div>
<div id="clean-the-data" class="section level2">
<h2>Clean the Data</h2>
<p>The first thing we notice are non-ascii characters, numbers, and non-words. We can clean these pretty quickly with a few lines.</p>
<pre class="r"><code>Encoding(Samples$text) &lt;- &quot;latin1&quot;
Samples$text &lt;- iconv(Samples$text, &quot;latin1&quot;, &quot;ASCII&quot;, sub = &quot;&quot;)    ## Remove Non-Ascii characters
Samples$text &lt;- gsub(&quot;\\d&quot;, &quot;&quot;, Samples$text)                       ## Remove numbers
Samples$text &lt;- gsub(&quot;\\W&quot;, &quot; &quot;, Samples$text)                      ## Remove Non-words</code></pre>
</div>
<div id="prepare-a-profanities-list" class="section level2">
<h2>Prepare a Profanities List</h2>
<p>We are going to want to remove profanity from the dataset, so that the most common swears don’t appear as predictions. Hardly ideal for a child to write an innocent string only to learn versatile and overused (especially on the internet) words. Will apply this list during tokenization.</p>
<pre class="r"><code>ProfanityList &lt;- &quot;H:/DataScience/Capstone/Data/Profanities/swearWords.txt&quot;
Profanities &lt;- readLines(con &lt;- file(ProfanityList, &quot;r&quot;), skipNul = TRUE)</code></pre>
</div>
<div id="tokenization" class="section level2">
<h2>Tokenization</h2>
<p>Now that we have a clean dataset without numbers, stange (Non-ASCII) characters, we need to do some tokenization. Breaking strings of text into individual words (unigrams), pairs (bigrams), or more (trigrams, quadgrams…) allows us to analyze the individual words and combinations. As far as a prediction algorithm, tokenization is critical. With a single word given, mining bigrams is the best way to predict the next word. With a couple words, bigrams and trigrams form the bases for prediction.</p>
<pre class="r"><code>## Merge profanities and stop_words
FilteredWords &lt;- c(stop_words$word, Profanities)

## Tokenize the words into unigrams and filter them
Gram1 &lt;- unnest_tokens(Samples, gram1, text, token = &quot;ngrams&quot;, n = 1)
  filtered1Gram &lt;- Gram1 %&gt;%                
    filter(!gram1 %in% FilteredWords)%&gt;%     ## Remove stop words and profanities
    filter(gram1 %in% GradyAugmented)        ## Remove words that don&#39;t appear in the dictionary
  filtered1GramCount &lt;- filtered1Gram %&gt;% count(gram1, sort = TRUE)     ## Count each unigram
  filtered1GramReunite &lt;- filtered1GramCount ## Not necessary, but a coding simplicity
  
## Tokenize the words into bigrams and filter them    
Gram2 &lt;- unnest_tokens(Samples, gram2, text, token = &quot;ngrams&quot;, n = 2)
  common2Grams &lt;- Gram2 %&gt;% count(gram2, sort = TRUE)
  separated2Grams &lt;- Gram2 %&gt;% separate(gram2, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)
  filtered2Grams &lt;- separated2Grams %&gt;%
    filter(!word1 %in% FilteredWords) %&gt;%     ## Remove stop words and profanities
    filter(word1 %in% GradyAugmented) %&gt;%     ## Remove words that don&#39;t appear in the dictionary
    filter(!word2 %in% FilteredWords) %&gt;%     ## Remove stop words and profanities
    filter(word2 %in% GradyAugmented)         ## Remove words that don&#39;t appear in the dictionary
  filtered2GramCount &lt;- filtered2Grams %&gt;% count(word1, word2, sort = TRUE)           ## Count each bigram
  filtered2GramReunite &lt;- unite(filtered2GramCount, gram2, word1, word2, sep = &quot; &quot;)   ## Reunite the bigrams together

## Tokenize the words into trigrams and filter them  
Gram3 &lt;- unnest_tokens(Samples, gram3, text, token = &quot;ngrams&quot;, n = 3)
  common3Grams &lt;- Gram3 %&gt;% count(gram3, sort = TRUE)
  separated3Grams &lt;- Gram3 %&gt;% separate(gram3, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;)
  filtered3Grams &lt;- separated3Grams %&gt;%
    filter(!word1 %in% FilteredWords) %&gt;%     ## Remove stop words and profanities
    filter(word1 %in% GradyAugmented) %&gt;%     ## Remove words that don&#39;t appear in the dictionary
    filter(!word2 %in% FilteredWords) %&gt;%     ## Remove stop words and profanities
    filter(word2 %in% GradyAugmented) %&gt;%     ## Remove words that don&#39;t appear in the dictionary
    filter(!word3 %in% FilteredWords) %&gt;%     ## Remove stop words and profanities
    filter(word3 %in% GradyAugmented)         ## Remove words that don&#39;t appear in the dictionary
  filtered3GramCount &lt;- filtered3Grams %&gt;% count(word1, word2, word3, sort = TRUE)            ## Count each trigram
  filtered3GramReunite &lt;- unite(filtered3GramCount, gram3, word1, word2, word3, sep = &quot; &quot;)    ## Reunite the trigrams together

  ## Cleaning up the names a bit, filterednGramReunite is a little bulky
  filtered1Gram &lt;- filtered1GramReunite
  filtered2Gram &lt;- filtered2GramReunite
  filtered3Gram &lt;- filtered3GramReunite

  ## Get rid of a few intermediate files.
  rm(filtered1GramReunite, filtered2GramReunite, filtered3GramReunite)
  rm(filtered1GramCount, filtered2GramCount, filtered3GramCount)
  rm(filtered2Grams, filtered3Grams)
  rm(separated2Grams, separated3Grams)
  
  filtered1Gram$Rank &lt;- seq.int(nrow(filtered1Gram))
  filtered2Gram$Rank &lt;- seq.int(nrow(filtered2Gram))
  filtered3Gram$Rank &lt;- seq.int(nrow(filtered3Gram))</code></pre>
</div>
<div id="visualizing-word-frequencies" class="section level2">
<h2>Visualizing Word Frequencies</h2>
<p>Lets compare the top ten most frequent unfiltered unigrams, bigrams and trigrams against their filtered equivalents. These are displayed here.</p>
<pre class="r"><code>#### WHAT ARE THE MOST COMMON UNFILTERED WORDS?
Gram1Frequency &lt;- Gram1 %&gt;% count(gram1, sort = TRUE)
Gram2Frequency &lt;- Gram2 %&gt;% count(gram2, sort = TRUE)
Gram3Frequency &lt;- Gram3 %&gt;% count(gram3, sort = TRUE)

## I want to be able to pick by rank
Gram1Frequency$Rank &lt;- seq.int(nrow(Gram1Frequency))
Gram2Frequency$Rank &lt;- seq.int(nrow(Gram2Frequency))
Gram3Frequency$Rank &lt;- seq.int(nrow(Gram3Frequency))

## Unfiltered Plots
Gram1FrequencyPlot &lt;- Gram1Frequency %&gt;%
  filter(Rank &lt; 11) %&gt;%
  mutate(gram1 = reorder(gram1, n)) %&gt;%
  ggplot(aes(gram1, n, fill = Rank)) +
  geom_col() + 
  coord_flip() + 
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;) + 
  ggtitle(&quot;Unfiltered&quot;)
Gram2FrequencyPlot &lt;- Gram2Frequency %&gt;%
  filter(Rank &lt; 11) %&gt;%
  mutate(gram2 = reorder(gram2, n)) %&gt;%
  ggplot(aes(gram2, n, fill = Rank)) +
  geom_col() + 
  coord_flip() +
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;)
Gram3FrequencyPlot &lt;- Gram3Frequency %&gt;%
  filter(Rank &lt; 11) %&gt;%
  mutate(gram3 = reorder(gram3, n)) %&gt;%
  ggplot(aes(gram3, n, fill = Rank)) +
  geom_col() + 
  coord_flip() +
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;)

## Filtered Plots
filtered1GramFrequencyPlot &lt;- filtered1Gram %&gt;%
  filter(Rank &lt; 11) %&gt;%
  mutate(gram1 = reorder(gram1, n)) %&gt;%
  ggplot(aes(gram1, n, fill = Rank)) +
  geom_col() + 
  coord_flip() + 
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;) + 
  labs(title = &quot;Filtered&quot;)
filtered2GramFrequencyPlot &lt;- filtered2Gram %&gt;%
  filter(Rank &lt; 11) %&gt;%
  mutate(gram2 = reorder(gram2, n)) %&gt;%
  ggplot(aes(gram2, n, fill = Rank)) +
  geom_col() + 
  coord_flip() +
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;)
filtered3GramFrequencyPlot &lt;- filtered3Gram %&gt;%
  filter(Rank &lt; 11) %&gt;%
  mutate(gram3 = reorder(gram3, n)) %&gt;%
  ggplot(aes(gram3, n, fill = Rank)) +
  geom_col() + 
  coord_flip() +
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;)

grid.arrange(Gram1FrequencyPlot, filtered1GramFrequencyPlot, Gram2FrequencyPlot, filtered2GramFrequencyPlot, Gram3FrequencyPlot, filtered3GramFrequencyPlot, nrow = 3)</code></pre>
<p><img src="Capstone_files/figure-html/nGram%20Analysis-1.png" width="960" /></p>
<pre class="r"><code>rm(Gram1FrequencyPlot, filtered1GramFrequencyPlot, Gram2FrequencyPlot, filtered2GramFrequencyPlot, Gram3FrequencyPlot, filtered3GramFrequencyPlot)</code></pre>
<p>It is pretty clear that the filter adds a lot of meaning. Unless one wants to always predict “the, to, I, a, and, of…” It is interesting that the most common trigram is happy mothers day, 20, 1. What are the relative frequencies of the entire dataset?</p>
<pre class="r"><code>fdGram1FrequencyPlot &lt;- Gram1Frequency %&gt;%
  filter(Rank &lt; 1001) %&gt;%
  mutate(gram1 = reorder(gram1, -n)) %&gt;%
  ggplot(aes(gram1, n, fill = Rank)) +
  geom_col() + 
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank()) + 
  ggtitle(&quot;Unfiltered Frequency of Unigrams - Distribution&quot;)

fdfiltered1GramFrequencyPlot &lt;- filtered1Gram %&gt;%
  filter(Rank &lt; 1001) %&gt;%
  mutate(gram1 = reorder(gram1, -n)) %&gt;%
  ggplot(aes(gram1, n, fill = Rank)) +
  geom_col() + 
  xlab(NULL) +
  ylab(NULL) +
  theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank()) + 
  labs(title = &quot;Filtered Frequency of Unigrams - Distribution&quot;)


grid.arrange(fdGram1FrequencyPlot, fdfiltered1GramFrequencyPlot)</code></pre>
<p><img src="Capstone_files/figure-html/EntireDataset%20Frequencies-1.png" width="960" /></p>
<pre class="r"><code>rm(fdGram1FrequencyPlot, fdfiltered1GramFrequencyPlot)</code></pre>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>We can see that there are a few very common words, either filtered or not, and then a long tail of words that are used occasionally or rarely. We have taken three distinct bodies of text (from Blogs, News and Twitter), broken out each word, and combination of two or three words, cleaned that data of non-word characters and evaluated their frequencies. Left at this stage, we are left with a list of “stop words” like the, and, it, but, if. These are the most common words to predict, but they are generally unimportant ones. If a user starts typing “Happy Mothers…” a quality prediction algorithm is not going to predict “The”… Hallmark probably needs to get a trademark for “Happy Mothers The.” Furthermore, “Just in the nick of AND” is probably not the most likely desired response.</p>
</div>
<div id="next-steps" class="section level2">
<h2>Next Steps</h2>
<ul>
<li>Now that we have a full picture of the n=3 Gram analysis, the next steps are to progress to prediction.</li>
<li>Improve and optimize the code, it takes an awfully long time to process everything just to get to trigram stage.</li>
<li>Experiment with larger nGrams and see if this produces more valuable insights.</li>
<li>Test some combinations of filtered and unfiltered. Currently the filters are a little bit overzealous - they remove all words from the analysis. I think I’m going to test with filters that only remove pure stop words, but leave those that contain at least one meaningful word.</li>
<li>Eventually I’m sure I’ll need to calculate probability. If a user types Happy Mothers… the program is going to need to say, “99% probability the next word is”Day“”.</li>
</ul>
<p>```</p>
<!-- ```{r Quiz #1 - Calculating Strings, eval=FALSE, include=FALSE} -->
<!-- ## QUESTION 1 : How big is the blogs text file? -->
<!-- file.info(dataEnglishBlogs)$size/1024/1024   ## Default reports in bytes, convert to kb (/1024), convert to mb (/1024) -->
<!-- ## 200.424 MB -->
<!-- ## QUESTION 2 : How many lines of twitter text? -->
<!-- length(Twitter)   ##2360148 -->
<!-- ## Easy way to calculate longest string length -->
<!-- # max(nchar(Twitter)) -->
<!-- # max(nchar(Blogs)) -->
<!-- # max(nchar(News)) -->
<!-- ## QUESTION 3 : What is the length of the longest line seen in the three data sets? -->
<!-- ## Faster and better way to calculate string length -->
<!-- max(stri_length(Twitter))   ## 213 (They are tweets afterall) -->
<!-- max(stri_length(Blogs))     ## 40835 <<< -->
<!-- max(stri_length(News))      ## 5760 -->
<!-- ## QUESTION 4 : What is the ratio of tweets that contain love vs hate? -->
<!-- ## Amor vincit odium -->
<!-- Love <- table(grepl("love", Twitter))   ## Creates a table identifying for each string whether they contain "love" -->
<!-- Hate <- table(grepl("hate", Twitter))   ## Creates a table identifying for each string whether they contain "hate" -->
<!-- LoveHate <- Love[2] / Hate[2]           ## Calculates the True Loves/True Hates -->
<!-- LoveHate                                ## 4.108592 -->
<!-- ## QUESTION 5 : There is one tweet about biostats, what is the subject talking about? -->
<!-- ## Biostats -->
<!-- grep("biostats", Twitter, value = T)    ## Searches for "biostats" if value is found, returns that string -->
<!-- ## "i know how you feel.. i have biostats on tuesday and i have yet to study =/" -->
<!-- ## QUESTION 6 : A very particular tweet is repeated how many times? -->
<!-- ## Computer Kickboxing -->
<!-- grep("A computer once beat me at chess, but it was no match for me at kickboxing", Twitter, value = T) -->
<!-- ## Result shows up three times - no other text at all, too bad, context might have been amusing. -->
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
